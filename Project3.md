## **Capstone Project 3**

### **Project Title:**
Customer Conversational Intelligence Platform Powered by an LLM Agent

### **Project Overview:**
This project aims to develop a Customer Conversational Intelligence Platform powered by a Large Language Model (LLM) agent (e.g., GPT-3/4). The platform will analyze customer interactions across diverse channels (e.g., chatbots, call centers, emails, social media), extracting actionable insights such as sentiment, intent, topic modeling, and agent performance evaluations. It will help businesses optimize their customer service processes and improve the overall customer experience.

### **Dataset:**
1. **Relational Strategies in Customer Interactions (RSiCS)**:
   - This dataset contains customer conversations for training intelligent virtual agents.
2. **3K Conversations Dataset for ChatBot from Kaggle**:
   - This dataset contains a variety of conversation types such as formal, casual, and customer service interactions.
3. **Customer Support on Twitter Dataset from Kaggle**:
   - A corpus of tweets and replies that can help in building conversational models.

---

### **Step-by-Step Installation and Deployment on GKE**

### **Step 1: Set Up Google Cloud Environment**

1. **Create a Google Cloud Project**:
   - Go to the Google Cloud Console and create a new project.
   - Enable billing for the project.

2. **Enable Required APIs**:
   - Enable the following APIs in the **API & Services** > **Library** section:
     - Kubernetes Engine API
     - Cloud Storage API
     - Cloud Natural Language API
     - Compute Engine API
     - Cloud Logging API

3. **Install Google Cloud SDK**:
   - Install the [Google Cloud SDK](https://cloud.google.com/sdk/docs/install) and initialize it:
     ```bash
     gcloud init
     ```

---

### **Step 2: Create and Configure a GKE Cluster**

1. **Create a GKE Cluster**:
   ```bash
   gcloud container clusters create conversational-platform-cluster \
       --zone us-central1-a \
       --num-nodes 3 \
       --machine-type n1-standard-4
   ```

2. **Connect to the GKE Cluster**:
   ```bash
   gcloud container clusters get-credentials conversational-platform-cluster --zone us-central1-a
   ```

---

### **Step 3: Prepare Application Code for Deployment**

1. **Set Up Project Directory**:
   Create a project directory:
   ```bash
   mkdir conversational-intelligence-platform
   cd conversational-intelligence-platform
   ```

2. **Create a Dockerfile**:
   Build a `Dockerfile` to containerize the application:
   ```dockerfile
   FROM python:3.8-slim

   # Set working directory
   WORKDIR /app

   # Install necessary packages
   COPY requirements.txt .
   RUN pip install --no-cache-dir -r requirements.txt

   # Copy application code
   COPY . .

   # Expose the necessary port
   EXPOSE 5000

   # Start the app
   CMD ["gunicorn", "--bind", "0.0.0.0:5000", "app:app"]
   ```

3. **Create the Requirements File**:
   Create a `requirements.txt` file with the necessary libraries:
   ```plaintext
   Flask
   transformers
   torch
   scikit-learn
   pandas
   numpy
   gunicorn
   prometheus_flask_exporter
   ```

4. **Application Code**:
   Create `app.py` for loading the fine-tuned LLM agent, analyzing conversations, and returning results:
   ```python
   from flask import Flask, request, jsonify
   from transformers import pipeline
   import joblib

   app = Flask(__name__)

   # Load pre-trained language model for sentiment analysis, intent recognition, etc.
   sentiment_analyzer = pipeline('sentiment-analysis')
   intent_classifier = joblib.load('intent_model.pkl')

   @app.route('/analyze', methods=['POST'])
   def analyze():
       data = request.get_json(force=True)
       text = data['conversation']

       # Sentiment Analysis
       sentiment = sentiment_analyzer(text)[0]

       # Intent Recognition
       intent = intent_classifier.predict([text])[0]

       return jsonify({
           'sentiment': sentiment,
           'intent': intent
       })

   if __name__ == '__main__':
       app.run(host='0.0.0.0', port=5000)
   ```

5. **Model Training and Saving**:
   Use separate scripts to train models for intent classification, topic modeling, and agent performance evaluation. Save models with joblib (`intent_model.pkl`) for integration into the Flask app.

---

### **Step 4: Build and Push Docker Image to Google Container Registry**

1. **Build the Docker Image**:
   ```bash
   docker build -t gcr.io/YOUR_PROJECT_ID/conversational-platform:v1 .
   ```

2. **Push the Docker Image to GCR**:
   ```bash
   docker push gcr.io/YOUR_PROJECT_ID/conversational-platform:v1
   ```

---

### **Step 5: Deploy Application on GKE**

1. **Create Deployment YAML**:
   Create `deployment.yaml` to deploy the app on GKE:
   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: conversational-platform
   spec:
     replicas: 3
     selector:
       matchLabels:
         app: conversational-platform
     template:
       metadata:
         labels:
           app: conversational-platform
       spec:
         containers:
         - name: conversational-platform
           image: gcr.io/YOUR_PROJECT_ID/conversational-platform:v1
           ports:
           - containerPort: 5000
   ```

2. **Apply the Deployment**:
   ```bash
   kubectl apply -f deployment.yaml
   ```

3. **Create Service to Expose the Application**:
   Create `service.yaml`:
   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: conversational-service
   spec:
     type: LoadBalancer
     ports:
       - port: 80
         targetPort: 5000
     selector:
       app: conversational-platform
   ```

4. **Apply the Service**:
   ```bash
   kubectl apply -f service.yaml
   ```

---

### **Step 6: Monitoring and Scaling**

1. **Set Up Prometheus and Grafana**:
   - Install Prometheus and Grafana on GKE to monitor the performance and health of the LLM-powered application.
   - Set up metrics like latency, request rate, and error rate.

2. **Enable Cloud Monitoring**:
   - Use Google Cloudâ€™s **Operations Suite** (formerly Stackdriver) for centralized logging, monitoring, and alerting.

3. **Scaling**:
   - Use Kubernetes Horizontal Pod Autoscaler (HPA) to automatically scale the number of replicas based on CPU utilization:
   ```bash
   kubectl autoscale deployment conversational-platform --cpu-percent=80 --min=3 --max=10
   ```

---

### **Step 7: Continuous Deployment & Retraining**

1. **Model Retraining**:
   - Set up a CI/CD pipeline with GitHub Actions, Jenkins, or Google Cloud Build to automate retraining and deployment.
   - Use scheduled retraining of the LLM models with fresh data (e.g., daily, weekly).

2. **Integrating Feedback**:
   - Implement a feedback loop in the platform where agents can provide feedback on model performance, allowing continuous refinement.

---
